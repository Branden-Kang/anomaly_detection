{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYPTHbZxTsgC3bQ9jSnHPG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://github.com/navdeep-G/robust-random-cut-forest)"
      ],
      "metadata": {
        "id": "ocIn42GBSKxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a normally distributed dataset of dimension [n, p] called X\n",
        "# this is normal non-anomalous data\n",
        "n = 1000\n",
        "p = 20\n",
        "X = np.random.randn(n * p).reshape(n, p)"
      ],
      "metadata": {
        "id": "9YVjOnu9Q7Se"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now add anomalies to the dataset\n",
        "outlier_prob = 0.05\n",
        "is_outlier = np.random.rand(n) > 0.95\n",
        "n_outliers = np.sum(is_outlier)\n",
        "X[is_outlier] = 3 * np.random.rand(n_outliers * p).reshape(n_outliers, p)"
      ],
      "metadata": {
        "id": "A2gDMnw-RBZQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EJHtnuOS32L",
        "outputId": "851d83d7-3a68-4179-992d-e59b9a9deca0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[is_outlier].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqQDbzTIS7gY",
        "outputId": "5175a25a-f816-4723-d80a-06659888ef1b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('is_outlier: ', is_outlier)\n",
        "print('The number of outliers: ', n_outliers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXXGRod2Sd-6",
        "outputId": "e9408aea-dd83-45a3-cb1d-e1a330ade413"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_outlier:  [False False False  True False False False False False False False False\n",
            " False False False False False False False False False False  True False\n",
            " False  True False False False False False False False False False False\n",
            " False False False False False  True False False False False False False\n",
            " False False False False False False False  True False False False False\n",
            " False False False False  True False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False  True False False  True False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False  True False False False False False\n",
            " False False False False False False False  True False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False  True False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False  True\n",
            " False False False False False False False False False False  True False\n",
            "  True False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False  True False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            "  True False False False  True False False False False False False False\n",
            " False False False False False False False False  True False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            "  True False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False  True False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False  True  True False False False False False False\n",
            " False False False False False  True False False False False False False\n",
            " False False False False False False False False False False False False\n",
            "  True False False  True False False False False False False False False\n",
            "  True False False  True  True False False False False False False False\n",
            " False  True False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False  True False False False False False\n",
            " False False False False False  True False False False False False False\n",
            " False False False False False False False False False  True False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False  True False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False  True False  True False False False\n",
            " False False  True False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False  True False False False False False False  True False False\n",
            "  True False False False False False False False False False False False\n",
            " False False  True False False False False False False False False False\n",
            " False False False False False  True False False False False False False\n",
            " False False False  True False False False False False False False False\n",
            " False  True False False False False  True False False False  True False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            "  True False False False  True False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False  True False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False  True False False\n",
            " False False False  True False False False False False False False  True\n",
            " False False False  True False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False  True False False  True False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False  True False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False]\n",
            "The number of outliers:  55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R1PDbxzSaOx",
        "outputId": "6725bfea-fd0f-4c49-db4d-3a987bd2832e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.28484191, -0.94059523,  1.58058859, ..., -0.32521655,\n",
              "        -0.84772037,  0.51412626],\n",
              "       [-0.11202373, -0.90934959,  1.45638293, ...,  0.48836255,\n",
              "         0.29273815,  0.04639294],\n",
              "       [-0.31604327, -0.33889243,  0.24353321, ..., -1.14079656,\n",
              "        -0.56513728,  0.71190933],\n",
              "       ...,\n",
              "       [ 1.00306091, -0.18608432, -0.30426958, ...,  0.07699352,\n",
              "         1.16478641, -0.40417652],\n",
              "       [-0.53586386, -0.47283779,  0.86015287, ..., -2.46798289,\n",
              "         0.95511284,  0.21806112],\n",
              "       [-0.10452085,  0.62861124,  1.35956493, ...,  0.90939772,\n",
              "         0.67034991,  0.39572598]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWCq49wNLcQ_"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from warnings import warn\n",
        "from sklearn import metrics\n",
        "\n",
        "class RobustRandomCutForest(object):\n",
        "    \"\"\"Robust Random Cut Forest\n",
        "\n",
        "    Return the anomaly score of each sample using the Robust  Random Cut Forest algorithm\n",
        "    The Robust Random Cut Forest 'isolates' observations by randomly selecting a feature\n",
        "    with probability proportional to its range and then uniformly selecting a split\n",
        "    at random between the maximum and minimum values of the selected feature.\n",
        "    Since recursive partitioning can be represented by a tree structure, the\n",
        "    number of splittings required to isolate a sample is equivalent to the path\n",
        "    length from the root node to the terminating node.\n",
        "    This path length, averaged over a forest of such random trees, is a\n",
        "    measure of abnormality and our decision function.\n",
        "    Random partitioning produces noticeably shorter paths for anomalies.\n",
        "    Hence, when a forest of random trees collectively produce shorter path\n",
        "    lengths for particular samples, they are highly likely to be anomalies.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int, optional (default=100)\n",
        "        The number of base estimators in the ensemble.\n",
        "\n",
        "    max_samples : int or float, optional (default=\"auto\")\n",
        "        The number of samples to draw from X to train each base estimator.\n",
        "            - If int, then draw `max_samples` samples.\n",
        "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
        "        If max_samples is larger than the number of samples provided,\n",
        "        all samples will be used for all trees (no sampling).\n",
        "\n",
        "    threshold : float in (0., 0.5), optional (default=0.25)\n",
        "        The threshold of the score used to determine outliers.  Lower means\n",
        "        fewer points are considered outliers.  This can be calculated to\n",
        "        allow for an expected proportion of points to be considered outliers\n",
        "        through the contamination_pct parameter and setting calculate_threshold\n",
        "        to be True during fitting.\n",
        "\n",
        "    contamination_pct : float in (0., 0.5), optional (default=0.01)\n",
        "        The amount of contamination of the data set, i.e. the proportion\n",
        "        of outliers in the data set. Used when fitting to define the threshold\n",
        "        on the decision function.\n",
        "\n",
        "    bootstrap : boolean, optional (default=False)\n",
        "        If True, individual trees are fit on random subsets of the training\n",
        "        data sampled with replacement. If False, sampling without replacement\n",
        "        is performed.\n",
        "\n",
        "    random_features : boolean, optional (default=False)\n",
        "        If True, the feature a tree splits on is chosen uniformaly at random\n",
        "        (among all features for which the range is nonzero) rather than\n",
        "        proportional to its range.\n",
        "\n",
        "    float_min : float in (0, infinity), optional (default=np.finfo(np.float64).eps*10000)\n",
        "        The minimum range for a feature to be considered worth splitting. If\n",
        "        all features have ranges smaller than this, no more splits will be made.\n",
        "        This is put in place to deal with floating point errors where all points\n",
        "        can land on the same side of a split.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_samples=256, max_node_depth=None, threshold=0.7, contamination_pct=.01, bootstrap=False, random_features=False, float_min=np.finfo(np.float64).eps * 10000):\n",
        "        self._n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.threshold = threshold\n",
        "        self.contamination_pct = contamination_pct\n",
        "        self.bootstrap = bootstrap\n",
        "        self.random_features = random_features\n",
        "        self.float_min = float_min\n",
        "        self.max_node_depth = max_node_depth\n",
        "\n",
        "    def fit(self, X, calculate_threshold=False):\n",
        "        '''Fit estimator.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
        "            The input samples. Use ``dtype=np.float32`` for maximum\n",
        "            efficiency. Sparse matrices are also supported, use sparse\n",
        "            ``csc_matrix`` for maximum efficiency.\n",
        "\n",
        "        calculate_threshold : boolean, optional (default=False)\n",
        "            If True, the treshold for an outlier used in the predict\n",
        "            function is calculated by computing the scores of the training\n",
        "            data and choosing the threshold to be the number at which\n",
        "            the specified contamination percent proportion of samples are\n",
        "            considered outliers.  Otherwise, the threshold specified when\n",
        "            initializing the forest is used.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        '''\n",
        "        self._n = X.shape[0]\n",
        "        self.max_samples_ = _get_max_samples(self.max_samples, self._n)\n",
        "        avg_depth = average_path_length(self.max_samples_)\n",
        "        # child_rightfit\n",
        "        self.trees = [\n",
        "            RandomCutTree(\n",
        "                random_features=self.random_features,\n",
        "                float_min=self.float_min,\n",
        "                max_node_depth=self.max_node_depth,\n",
        "                avg_depth=avg_depth\n",
        "            ).fit(_sample(X, self.max_samples_, self.bootstrap))\n",
        "            for i in range(self._n_estimators)\n",
        "        ]\n",
        "\n",
        "        # calculate the threshold for outliers by evaluating the scores of the\n",
        "        # training set\n",
        "        if calculate_threshold:\n",
        "            self.threshold = \\\n",
        "                -stats.scoreatpercentile(-self.decision_function(X),\n",
        "                                         100. * (1. - self.contamination_pct))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Predict if a particular sample is an outlier or not.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
        "            The input samples. Internally, it will be converted to\n",
        "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
        "            to a sparse ``csr_matrix``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        is_inlier : array, shape (n_samples,)\n",
        "            For each observation, tells whether or not (+1 or -1) it should\n",
        "            be considered as an inlier according to the fitted model.\n",
        "        '''\n",
        "        scores = self.decision_function(X)\n",
        "        is_inlier = np.zeros(X.shape[0], dtype=int)\n",
        "        is_inlier[scores <= self.threshold] = 1\n",
        "        return is_inlier\n",
        "\n",
        "    def decision_function(self, X, transformed=True):\n",
        "        '''Average anomaly score of X of the base classifiers.\n",
        "\n",
        "        The anomaly score of an input sample is computed as\n",
        "        a function of the mean of its depth across all trees in the forest.\n",
        "        The depths of a leaf is equivalent to the number of splittings required\n",
        "        to isolate this point.  In case of several identical observations in the\n",
        "        leaf, the average path length required to separate that many points is\n",
        "        added to the length.  These default is to apply a transformation to the\n",
        "        depths of the leaves to return a more easily interpretable score in the\n",
        "        range (0, 1).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "\n",
        "        transformed : boolean, optional (default=False)\n",
        "            If True, the score is transformed to lie in the range (0, 1) where\n",
        "            high scores represent normal points and low scores represent anomalies.\n",
        "            If False, the average tree depth is returned.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        scores : array of shape (n_samples,)\n",
        "            The anomaly score of the input samples.\n",
        "            The lower, the more abnormal.\n",
        "        '''\n",
        "        depths = np.column_stack([tree.decision_function(X)\n",
        "                                  for tree in self.trees])\n",
        "        mean_depths = np.mean(depths, axis=1)\n",
        "\n",
        "        if transformed:\n",
        "            scores = np.power(2., -mean_depths /\n",
        "                              average_path_length(self.max_samples_))\n",
        "        else:\n",
        "            scores = mean_depths\n",
        "\n",
        "        return 0.5 - scores\n",
        "\n",
        "    def add_point(self, x):\n",
        "        '''add a point to the forest\n",
        "\n",
        "        This method streams a point into the forest.  The point is added and\n",
        "        and existing point is removed randomly based on reservoir sampling.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : {array-like, sparse matrix}, shape (n_features,)\n",
        "            An individual training sample\n",
        "        '''\n",
        "        self._n += 1\n",
        "        add_point_probability = np.float(self.max_samples_) / self._n\n",
        "        should_add_points = np.random.binomial(\n",
        "            1, add_point_probability, size=self._n_estimators)\n",
        "        for tree, should_add_point in zip(self.trees, should_add_points):\n",
        "            if should_add_point == 1:\n",
        "                if tree._X.shape[0] > 1:\n",
        "                    tree.remove_point()\n",
        "                tree.add_point(x)\n",
        "\n",
        "\n",
        "class RandomCutTree(object):\n",
        "    '''Random Cut Tree\n",
        "\n",
        "    An individual tree in a random cut forest.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    random_features : boolean, optional (default=False)\n",
        "        If True, the feature a tree splits on is chosen uniformaly at random\n",
        "        (among all features for which the range is nonzero) rather than\n",
        "        proportional to its range.\n",
        "\n",
        "    float_min : float in (0, infinity), optional (default=np.finfo(np.float64).eps*10000)\n",
        "        The minimum range for a feature to be considered worth splitting. If\n",
        "        all features have ranges smaller than this, no more splits will be made.\n",
        "        This is put in place to deal with floating point errors where all points\n",
        "        can land on the same side of a split.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, avg_depth=None, max_node_depth=None, random_features=False, float_min=np.finfo(np.float64).eps * 10000):\n",
        "        self.random_features = random_features\n",
        "        self.float_min = float_min\n",
        "        self.max_node_depth = max_node_depth\n",
        "        self._avg_depth = avg_depth\n",
        "\n",
        "    def _update_vector(self, X):\n",
        "        self._X = X\n",
        "        self._X.setflags(write=False)  # Ensure the base array doesn't change\n",
        "\n",
        "    def fit(self, X):\n",
        "        '''fit a random cut tree'''\n",
        "        self._update_vector(X=X)\n",
        "\n",
        "        feature_mins = np.min(self._X, axis=0)\n",
        "        feature_maxs = np.max(self._X, axis=0)\n",
        "        feature_data = (feature_mins, feature_maxs)\n",
        "\n",
        "        self._root = TreeNode(\n",
        "            n=self._X.shape[0],\n",
        "            parent=None,\n",
        "            feature_data=feature_data,\n",
        "            random_features=self.random_features,\n",
        "            float_min=self.float_min\n",
        "        )\n",
        "        self.split_node(node=self._root, X=X)\n",
        "        return self\n",
        "\n",
        "    def split_node(self, node, X, current_depth=0):\n",
        "        if self.max_node_depth is not None and current_depth >= self.max_node_depth:\n",
        "            node._X = X\n",
        "        elif not node.is_leaf:\n",
        "            (child_left, child_right, X_left, X_right) = node._split(X)\n",
        "            self.split_node(child_left, X_left, current_depth + 1)\n",
        "            self.split_node(child_right, X_right, current_depth + 1)\n",
        "        else:\n",
        "            node._X = X\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        '''return the decision function (the depth) for each point in the tree'''\n",
        "        return np.array([self._root.get_depth(x, self.max_node_depth, self._avg_depth) for x in X])\n",
        "\n",
        "    def add_point(self, x):\n",
        "        '''insert a new point into the tree'''\n",
        "        self._update_vector(np.row_stack([self._X, x]))\n",
        "        self._root = self.add_new_node(\n",
        "            node=self._root, point=x, current_depth=0)\n",
        "\n",
        "    def remove_point(self):\n",
        "        '''forget a point at random from the tree'''\n",
        "\n",
        "        choice = np.random.choice(self._root.num_points())\n",
        "        point_to_forget = self._X[choice]\n",
        "\n",
        "        current_point_index = np.argmax(\n",
        "            np.all(self._X == point_to_forget, axis=1))\n",
        "        self._X = np.delete(self._X, current_point_index, 0)\n",
        "        self._root = self.remove_node(node=self._root, point=point_to_forget)\n",
        "\n",
        "    def add_new_node(self, node, point, current_depth=0):\n",
        "        '''insert a new point into the tree'''\n",
        "        if node is None:\n",
        "            return TreeNode(n=1, parent=None, feature_data=(point, point))\n",
        "\n",
        "        # pick a split for if the new point is included\n",
        "        (feature_mins, feature_maxs) = node.get_feature_ranges(point)\n",
        "        feature_ranges = feature_maxs - feature_mins\n",
        "\n",
        "        (alone_left, alone_right, split_data) = \\\n",
        "            node.get_new_node_position(feature_ranges, feature_mins)\n",
        "\n",
        "        if self.max_node_depth is not None and current_depth >= self.max_node_depth:\n",
        "            node.feature_data = (feature_mins, feature_maxs)\n",
        "            node.point_added()\n",
        "\n",
        "        # Check if the new split separates the new point from the node\n",
        "        # If it does we will use this new split\n",
        "        elif alone_left or alone_right:\n",
        "            child = node\n",
        "            node = TreeNode(\n",
        "                n=child.num_points() + 1,\n",
        "                parent=node.parent,\n",
        "                feature_data=(feature_mins, feature_maxs),\n",
        "                split_data=split_data,\n",
        "                random_features=child.random_features,\n",
        "                float_min=child.float_min\n",
        "            )\n",
        "\n",
        "            if alone_left:\n",
        "                node.child_right = child\n",
        "                node.child_left = TreeNode(\n",
        "                    n=1,\n",
        "                    parent=node,\n",
        "                    feature_data=(point, point),\n",
        "                    is_left_of_parent=True,\n",
        "                    float_min=node.float_min\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                node.child_left = child\n",
        "                node.child_right = TreeNode(\n",
        "                    n=1,\n",
        "                    parent=node,\n",
        "                    feature_data=(point, point),\n",
        "                    is_left_of_parent=False,\n",
        "                    float_min=node.float_min\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            # update node statistics to include new point\n",
        "            node.feature_data = (feature_mins, feature_maxs)\n",
        "            node.point_added()\n",
        "            # find the new point's leaf starting at the node below\n",
        "            # note that we use the old splitting criterion\n",
        "\n",
        "            if node.is_point_left(point):\n",
        "                node.child_left = self.add_new_node(\n",
        "                    node.child_left, point, current_depth + 1)\n",
        "                node.child_left.parent = node\n",
        "            else:\n",
        "                node.child_right = self.add_new_node(\n",
        "                    node.child_right, point, current_depth + 1)\n",
        "                node.child_right.parent = node\n",
        "\n",
        "        return node\n",
        "\n",
        "    def remove_node(self, node, point):\n",
        "        '''forget a point from the tree'''\n",
        "        if node.is_leaf:\n",
        "            return self._remove_from_leaf(node, point)\n",
        "\n",
        "        node.point_removed()\n",
        "        prev_parent = node.parent\n",
        "        if node.is_point_left(point):\n",
        "            node.child_left = self.remove_node(node.child_left, point)\n",
        "            if node.child_left is None:\n",
        "                node = node.child_right\n",
        "                node.parent = prev_parent\n",
        "\n",
        "        else:\n",
        "            node.child_right = self.remove_node(node.child_right, point)\n",
        "\n",
        "            if node.child_right is None:\n",
        "                node = node.child_left\n",
        "                node.parent = prev_parent\n",
        "\n",
        "        return node\n",
        "\n",
        "    def _remove_from_leaf(self, node, point):\n",
        "        if node.num_points() > 1:  # array is duplicate points\n",
        "            node.point_removed()\n",
        "            return node\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class TreeNode(object):\n",
        "    '''Tree Node\n",
        "\n",
        "    An individual node in a random cut tree.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n, parent, is_left_of_parent=None, feature_data=None, split_data=None, child_left=None, child_right=None, initialize=False, random_features=False, float_min=np.finfo(np.float64).eps * 10000):\n",
        "        self._n = n\n",
        "        self.parent = parent\n",
        "        self.is_left_of_parent = is_left_of_parent\n",
        "        self.random_features = random_features\n",
        "        self.float_min = float_min\n",
        "\n",
        "        self.feature_data = feature_data\n",
        "        self.child_left = child_left\n",
        "        self.child_right = child_right\n",
        "        self._X = None\n",
        "\n",
        "        self._set_split()\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self):\n",
        "        \"\"\" Ensures we correctly check if a node is a leaf. \"\"\"\n",
        "        return np.all(self.feature_ranges < self.float_min)\n",
        "\n",
        "    @property\n",
        "    def feature_ranges(self):\n",
        "        return self.feature_maxs - self.feature_mins\n",
        "\n",
        "    @property\n",
        "    def feature_mins(self):\n",
        "        return self.feature_data[0]\n",
        "\n",
        "    @property\n",
        "    def feature_maxs(self):\n",
        "        return self.feature_data[1]\n",
        "\n",
        "    @feature_mins.setter\n",
        "    def feature_mins(self, val):\n",
        "        self.feature_data = (val, self.feature_data[1])\n",
        "\n",
        "    @feature_maxs.setter\n",
        "    def feature_maxs(self, val):\n",
        "        self.feature_data = (self.feature_data[0], val)\n",
        "\n",
        "    def is_point_left(self, point):\n",
        "        if point[self.split_feature] < self.split_threshold:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_new_node_position(self, feature_ranges, feature_mins):\n",
        "        if np.all(feature_ranges < self.float_min):\n",
        "            # Node is a leaf and we don't want to continue down this path\n",
        "            return (False, False, (None, None))\n",
        "\n",
        "        split_feature = self._sample_split_feature(feature_ranges)\n",
        "        split_threshold = self._sample_split_threshold(\n",
        "            feature_ranges, feature_mins, split_feature)\n",
        "\n",
        "        alone_left = self.feature_mins[split_feature] > split_threshold\n",
        "\n",
        "        alone_right = self.feature_maxs[split_feature] < split_threshold\n",
        "\n",
        "        return (alone_left, alone_right, (split_feature, split_threshold))\n",
        "\n",
        "    def point_added(self):\n",
        "        \"\"\" Wrapper so the number of points isn't directly exposed \"\"\"\n",
        "        self._n += 1\n",
        "\n",
        "    def num_points(self):\n",
        "        \"\"\" Wrapper to get the number of points stored in a node. \"\"\"\n",
        "        return self._n\n",
        "\n",
        "    def point_removed(self):\n",
        "        \"\"\" Wrapper to decrement the number of points \"\"\"\n",
        "        self._n -= 1\n",
        "\n",
        "    def get_feature_ranges(self, point):\n",
        "        \"\"\" Compute the feature ranges given a new point \"\"\"\n",
        "        feature_mins = np.minimum(self.feature_mins, point)\n",
        "        feature_maxs = np.maximum(self.feature_maxs, point)\n",
        "        return (feature_mins, feature_maxs)\n",
        "\n",
        "    def _set_split(self):\n",
        "        '''set splitting feature and threshold'''\n",
        "        split_feature = self._sample_split_feature(self.feature_ranges)\n",
        "        split_threshold = self._sample_split_threshold(\n",
        "            self.feature_ranges, self.feature_mins, split_feature)\n",
        "        self.split_feature = split_feature\n",
        "        self.split_threshold = split_threshold\n",
        "\n",
        "    def _split(self, X):\n",
        "        '''split based on feature and threshold'''\n",
        "        is_left = X[:, self.split_feature] < self.split_threshold\n",
        "        X_left = X[is_left]\n",
        "        X_right = X[np.logical_not(is_left)]\n",
        "\n",
        "        feature_data_left = (np.min(X_left, axis=0), np.max(X_left, axis=0))\n",
        "        feature_data_right = (np.min(X_right, axis=0), np.max(X_right, axis=0))\n",
        "\n",
        "        self.child_left = TreeNode(\n",
        "            n=X_left.shape[0],\n",
        "            parent=self,\n",
        "            feature_data=feature_data_left,\n",
        "            is_left_of_parent=True,\n",
        "            float_min=self.float_min\n",
        "        )\n",
        "\n",
        "        self.child_right = TreeNode(\n",
        "            n=X_right.shape[0],\n",
        "            parent=self,\n",
        "            feature_data=feature_data_right,\n",
        "            is_left_of_parent=False,\n",
        "            float_min=self.float_min\n",
        "        )\n",
        "\n",
        "        return (self.child_left, self.child_right, X_left, X_right)\n",
        "\n",
        "    def _sample_split_feature(self, feature_ranges):\n",
        "        '''sample the feature to split on'''\n",
        "        if self.random_features:\n",
        "            is_feature_varying = np.array(\n",
        "                feature_ranges > self.float_min, dtype=float)\n",
        "            return np.flatnonzero(np.random.multinomial(1, is_feature_varying / np.sum(is_feature_varying)))[0]\n",
        "        else:\n",
        "            if np.sum(feature_ranges) == 0:\n",
        "                return np.flatnonzero(np.random.multinomial(1, feature_ranges))[0]\n",
        "            return np.flatnonzero(np.random.multinomial(1, feature_ranges / np.sum(feature_ranges)))[0]\n",
        "\n",
        "    def _sample_split_threshold(self, feature_ranges, feature_mins, split_feature):\n",
        "        '''sample the splitting threshold of a node'''\n",
        "        return np.random.rand() * feature_ranges[split_feature] + feature_mins[split_feature]\n",
        "\n",
        "    def get_depth(self, x, max_node_depth=None, avg_depth=None, current_depth=0):\n",
        "        '''calculate number of nodes to isolate a point'''\n",
        "        if max_node_depth is not None and current_depth >= max_node_depth:\n",
        "            return avg_depth\n",
        "        if self.is_leaf:\n",
        "            return current_depth + average_path_length(self._n)\n",
        "        elif x[self.split_feature] < self.split_threshold:\n",
        "            return self.child_left.get_depth(x, max_node_depth, avg_depth, current_depth + 1)\n",
        "        else:\n",
        "            return self.child_right.get_depth(x, max_node_depth, avg_depth, current_depth + 1)\n",
        "\n",
        "\n",
        "def _sample(X, num_samples, replace=False):\n",
        "    '''take a random sample of X'''\n",
        "    n = X.shape[0]\n",
        "    return X[np.random.choice(n, num_samples, replace)]\n",
        "\n",
        "\n",
        "def _get_max_samples(max_samples, n):\n",
        "    '''get the number of samples for each tree'''\n",
        "    if isinstance(max_samples, int):\n",
        "        if max_samples > n:\n",
        "            warn(\"max_samples (%s) is greater than the \"\n",
        "                 \"total number of samples (%s). max_samples \"\n",
        "                 \"will be set to n_samples for estimation.\"\n",
        "                 % (max_samples, n))\n",
        "            max_samples_ = n\n",
        "        else:\n",
        "            max_samples_ = max_samples\n",
        "    else:  # float\n",
        "        if not (0. < max_samples <= 1.):\n",
        "            raise ValueError(\"max_samples must be in (0, 1]\")\n",
        "        max_samples_ = int(max_samples * n)\n",
        "\n",
        "    return max_samples_\n",
        "\n",
        "\n",
        "def harmonic_approx(n):\n",
        "    '''Returns an approximate value of n-th harmonic number.'''\n",
        "    # Euler-Mascheroni constant\n",
        "    gamma = 0.57721566490153286060651209\n",
        "    return gamma + np.log(n) + 0.5 / n - 1. / (12 * np.square(n)) + 1. / (120 * np.power(n, 4))\n",
        "\n",
        "\n",
        "def average_path_length(n):\n",
        "    '''Returns the average path length of an unsuccessful search in a BST'''\n",
        "    return 2. * harmonic_approx(n - 1) - 2. * (n - 1.) / n if n > 1 else 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run a batch job to build a random cut forest to identify what the anomalies in the dataset are\n",
        "forest_batch = RobustRandomCutForest(max_samples=128, random_features=False).fit(X)"
      ],
      "metadata": {
        "id": "WadQiSQxRgHl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_batch = forest_batch.decision_function(X)"
      ],
      "metadata": {
        "id": "WvqK-_E8Rhnk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Forest batch: ', forest_batch)\n",
        "print('Scores batch: ', scores_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em13vG_TR1Ac",
        "outputId": "0e08960d-9a07-4866-a8dd-c071b0a0c1b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forest batch:  <__main__.RobustRandomCutForest object at 0x7ff868077710>\n",
            "Scores batch:  [ 0.07648815  0.12715913  0.02193005 -0.03128418  0.08240607  0.07048636\n",
            "  0.08759699  0.05684278  0.09144779  0.10833624  0.08044268  0.06779167\n",
            "  0.09462931  0.0398972   0.10216423  0.01968232  0.09747148  0.05196583\n",
            "  0.02267696  0.03953736  0.07149253  0.085658   -0.08582393  0.03519728\n",
            "  0.06745365 -0.07358795  0.06947783  0.08273241  0.02713395  0.07516171\n",
            "  0.05021109  0.09935519  0.08142553  0.0234227   0.06846692  0.1180129\n",
            "  0.03953736  0.05371373  0.03845616  0.10741658  0.08338432 -0.07493478\n",
            "  0.06779167  0.1095591   0.07648815  0.04312312  0.1095591   0.04490552\n",
            "  0.03008213  0.02416727  0.04133375  0.10802993  0.02491068  0.02639402\n",
            "  0.09399499 -0.07899429  0.11290233  0.06473993  0.0637179   0.01930667\n",
            "  0.11681652  0.11199339  0.05091381  0.07582545 -0.06956635  0.0718274\n",
            "  0.03845616  0.06063739  0.05196583  0.04133375  0.07879946  0.0637179\n",
            "  0.06947783  0.04490552  0.07082201  0.04205034  0.08759699  0.06098074\n",
            "  0.08984768  0.07748026  0.08727446  0.09462931  0.06846692  0.07283043\n",
            "  0.06677681 -0.08490867  0.0637179   0.07383111 -0.05506022  0.10029373\n",
            "  0.09176706  0.06846692  0.03737241  0.07945752  0.05994989  0.09240486\n",
            "  0.09904185  0.09208608  0.05231596  0.10772337  0.06405884  0.05475919\n",
            "  0.09208608  0.07149253  0.0908085  -0.01573316  0.04985932  0.04133375\n",
            "  0.09904185  0.085658    0.03337686  0.12305601  0.09144779  0.00326128\n",
            " -0.08996032  0.03446997  0.08888461  0.03337686  0.00981938  0.09841444\n",
            "  0.09240486  0.03628612  0.10680227  0.06405884  0.09272338  0.04985932\n",
            " -0.00024609 -0.06468933  0.05891661  0.07449693  0.07681911  0.08662863\n",
            "  0.09112827  0.12832322  0.09778605  0.05649619  0.06132383  0.00092578\n",
            "  0.08791927  0.09966828  0.02379513  0.06541995  0.07814038  0.06439952\n",
            "  0.04985932  0.12039448  0.03773394  0.03446997  0.10557078  0.08305849\n",
            "  0.06473993  0.07748026  0.07781045  0.01968232  0.03337686  0.07283043\n",
            "  0.03008213  0.11561639  0.05475919 -0.01251771  0.08727446  0.05684278\n",
            "  0.07383111  0.08175263  0.05822642  0.11138624  0.11380913  0.04985932\n",
            "  0.07648815  0.12217091 -0.01452501  0.05021109 -0.06292624  0.04880235\n",
            "  0.07416415  0.06473993  0.0837099   0.03773394  0.08436028  0.08468509\n",
            "  0.02043274  0.07748026  0.00905235  0.08468509  0.04738919  0.02155616\n",
            " -0.00417223 -0.0214088   0.02971461  0.05441098  0.04490552  0.12276121\n",
            "  0.06405884  0.10402598  0.05441098  0.12599139  0.08791927  0.07048636\n",
            "  0.10123008  0.10557078  0.06609891  0.07748026  0.09272338  0.07978616\n",
            "  0.04597161  0.13064051 -0.00733532  0.13693885  0.08109817  0.04950727\n",
            "  0.0326467   0.0105852   0.05441098  0.1101691   0.01477591 -0.08308246\n",
            "  0.03483377  0.07814038  0.03881684  0.07549371  0.10185309  0.04526116\n",
            "  0.06981427  0.08630534  0.085658    0.0398972  -0.05375994  0.02861034\n",
            " -0.04987736  0.05126476  0.10833624  0.07748026  0.07714982  0.08791927\n",
            "  0.11441249  0.06947783  0.10247513  0.06812943  0.08305849  0.09367746\n",
            "  0.09462931  0.12686753  0.00981938  0.0398972   0.07648815  0.07648815\n",
            "  0.05960573  0.07615693  0.00674407  0.06166664  0.07316425  0.06269347\n",
            "  0.10925375  0.03737241  0.00789957 -0.0455953   0.0859818   0.09684162\n",
            "  0.05891661  0.02787272  0.03483377  0.09240486  0.00828413  0.04240821\n",
            "  0.04561653  0.09431227  0.05822642  0.07582545  0.01325618  0.06303522\n",
            "  0.07879946  0.12246618  0.08240607  0.08759699  0.05994989  0.09494609\n",
            " -0.01775305  0.09872827  0.08273241  0.02043274 -0.08308246  0.02676413\n",
            "  0.07416415  0.06235146  0.07516171  0.04985932  0.03809519  0.0637179\n",
            "  0.08077055  0.02379513  0.06029378  0.07082201  0.07945752  0.05857165\n",
            "  0.00053546  0.06439952 -0.09925717  0.04738919  0.05822642  0.06303522\n",
            "  0.08403521  0.12423292  0.02713395  0.11199339  0.03845616  0.07482945\n",
            "  0.03917724  0.04809632  0.0398972  -0.00142071  0.05196583  0.08662863\n",
            "  0.09526263  0.08403521  0.11591677  0.05161543  0.0718274   0.03154932\n",
            "  0.07912862  0.05822642  0.02155616  0.06337669  0.0370106   0.03118295\n",
            " -0.05941656  0.08727446  0.06235146  0.06914112  0.02971461  0.10986422\n",
            "  0.02416727  0.08630534  0.10154171  0.06508007  0.06981427  0.04985932\n",
            "  0.04950727  0.04844947  0.05161543  0.08500965  0.04240821  0.10278579\n",
            "  0.05580221  0.06846692  0.03953736  0.00053546  0.14452203  0.11380913\n",
            "  0.07945752  0.04383692  0.05056259  0.13121758  0.07015045  0.09176706\n",
            "  0.06508007  0.11168993  0.01096767  0.05231596  0.06098074 -0.01091752\n",
            "  0.04668096  0.05891661  0.04240821 -0.04304209  0.03044935  0.13750607\n",
            "  0.11108231  0.05371373  0.05614934  0.10029373  0.05161543  0.07015045\n",
            "  0.0105852   0.0637179   0.05822642  0.07814038  0.03592346  0.12098755\n",
            "  0.07482945  0.06166664  0.11168993  0.07582545  0.09966828  0.04844947\n",
            "  0.05475919  0.03301193  0.02934681 -0.00102887 -0.03294816 -0.10490545\n",
            "  0.04490552  0.02416727  0.07015045  0.09494609  0.09272338  0.03845616\n",
            "  0.0326467   0.10154171  0.09112827  0.06711536  0.11471382 -0.08490867\n",
            " -0.00693885  0.03044935  0.06541995 -0.0113171   0.09272338  0.02230365\n",
            "  0.0895269   0.03118295  0.0366485   0.07449693  0.03519728  0.00866839\n",
            "  0.04383692  0.03301193  0.09526263  0.03881684  0.08240607  0.05822642\n",
            " -0.07179708  0.03917724  0.04205034 -0.05289478  0.10060609  0.06508007\n",
            "  0.05091381  0.08436028  0.01325618  0.03519728  0.04419339  0.06303522\n",
            " -0.04261772  0.07015045  0.10247513 -0.109653   -0.06468933  0.06132383\n",
            "  0.04526116  0.08759699  0.03773394  0.05126476  0.0895269   0.02304997\n",
            "  0.04348016 -0.11974449  0.0366485   0.0370106   0.09272338  0.04668096\n",
            "  0.07714982  0.09144779  0.07681911  0.06947783  0.08984768  0.07216201\n",
            "  0.0208075   0.0901682  -0.02222469  0.0398972   0.05891661  0.0718274\n",
            "  0.04312312  0.03881684  0.06880415  0.02750348 -0.0113171   0.06779167\n",
            "  0.09367746  0.07814038  0.07048636  0.07879946  0.09557892  0.09462931\n",
            " -0.06912125  0.01930667  0.10340637  0.04597161 -0.03839219  0.04561653\n",
            "  0.05231596  0.09935519  0.0477429   0.06508007  0.04844947 -0.04389183\n",
            "  0.07048636  0.07814038  0.04561653  0.11047374  0.05406249  0.06029378\n",
            "  0.0908085   0.09715667  0.09621076  0.0901682   0.06200918  0.07814038\n",
            "  0.01780114  0.01325618  0.09621076 -0.06778803  0.08109817  0.07383111\n",
            "  0.08305849  0.03881684  0.08500965  0.1101691   0.03228119  0.08695167\n",
            "  0.05649619  0.07714982  0.06235146  0.0533647   0.08207948  0.04097503\n",
            "  0.085658    0.04205034  0.06745365  0.09652631  0.00905235  0.0063583\n",
            "  0.0901682   0.09272338  0.05510714  0.04097503  0.07978616  0.01780114\n",
            "  0.04205034  0.0837099   0.06947783  0.10495358  0.1101691   0.10278579\n",
            "  0.04312312  0.09935519  0.06508007  0.1052623   0.0718274   0.04668096\n",
            "  0.05510714  0.0020949   0.03118295  0.11561639  0.05301539  0.06711536\n",
            " -0.00535607  0.0370106   0.05891661  0.05822642  0.0366485   0.03809519\n",
            "  0.03519728  0.06098074  0.01591259  0.06643799  0.07216201  0.03228119\n",
            "  0.0020949   0.09367746  0.07216201 -0.05506022  0.01968232  0.08077055\n",
            "  0.04632643  0.09304166  0.09240486  0.07912862  0.09589497  0.04061603\n",
            "  0.03592346 -0.03045413  0.05126476  0.1215797   0.11047374  0.07482945\n",
            "  0.03044935  0.04348016  0.06235146  0.04668096  0.05021109  0.07978616\n",
            " -0.03881326  0.0895269  -0.01011929  0.05475919  0.09715667  0.05056259\n",
            "  0.02304997  0.0445496  -0.09088347  0.10340637  0.07416415  0.0445496\n",
            " -0.03086899  0.10091821  0.03809519  0.04205034  0.06166664  0.13835525\n",
            "  0.11531576  0.05580221  0.04169218  0.05091381  0.0718274   0.05684278\n",
            " -0.01694414  0.06200918  0.05788091  0.02602362  0.07449693  0.07449693\n",
            "  0.02750348  0.04738919 -0.07673548  0.02565293  0.07978616  0.01666889\n",
            "  0.08338432  0.08305849  0.05126476 -0.03629178  0.04703521  0.09526263\n",
            " -0.06204676  0.02971461  0.07549371  0.15384632  0.08533395  0.06303522\n",
            "  0.08630534 -0.01734844  0.07349781  0.08240607  0.07149253  0.10154171\n",
            "  0.04950727  0.03044935 -0.08308246  0.10216423  0.05475919  0.03228119\n",
            "  0.10464462  0.00866839  0.08500965  0.05196583  0.09589497  0.11890774\n",
            "  0.03737241  0.00442494 -0.00417223  0.09589497  0.10278579 -0.0300396\n",
            "  0.07945752  0.04312312  0.07945752  0.09399499  0.03845616  0.07912862\n",
            "  0.01477591  0.09431227  0.09998113 -0.11299844  0.06405884  0.06132383\n",
            "  0.04276581  0.03773394  0.00403736  0.06745365  0.07681911  0.06166664\n",
            "  0.04240821 -0.00220533  0.11860969  0.06812943  0.07082201  0.08920588\n",
            " -0.04516894  0.13265629  0.07383111  0.04668096 -0.04092354  0.07149253\n",
            "  0.03773394  0.04703521  0.07082201  0.07449693  0.0117317   0.07383111\n",
            "  0.07582545  0.07582545  0.05649619  0.10495358  0.03118295  0.0445496\n",
            "  0.08984768  0.08044268  0.05266581  0.07748026  0.06779167  0.00866839\n",
            "  0.13380322  0.02230365 -0.02304186  0.09335968  0.06779167  0.05510714\n",
            " -0.02018736  0.01401664  0.09272338  0.10986422 -0.01896878  0.0637179\n",
            "  0.08044268  0.07149253  0.06508007  0.05788091  0.11077815  0.05822642\n",
            "  0.08240607 -0.02509037  0.05510714  0.06473993  0.05301539  0.05753514\n",
            "  0.00403736 -0.02100134  0.11950313  0.01968232  0.06337669  0.04276581\n",
            "  0.08856308  0.04668096  0.04844947  0.07249635  0.1122966   0.10154171\n",
            "  0.01666889  0.05056259  0.05441098  0.05753514 -0.08857829  0.04061603\n",
            "  0.04061603  0.0370106   0.0151551   0.04880235  0.07216201  0.04490552\n",
            "  0.07847005  0.09208608  0.05266581  0.03809519  0.05926131  0.05580221\n",
            "  0.05614934  0.11681652  0.07714982  0.06098074  0.09998113  0.10278579\n",
            "  0.05510714  0.08403521  0.06846692 -0.08171654  0.03737241  0.0234227\n",
            "  0.07316425  0.03446997  0.09272338 -0.03713096  0.0370106   0.03301193\n",
            "  0.05718909  0.01249454  0.11168993  0.07082201  0.12069113 -0.03503548\n",
            "  0.07283043  0.07748026  0.11501491 -0.0292115   0.01020244  0.05857165\n",
            "  0.02861034  0.09144779  0.05788091  0.08920588  0.08695167  0.04383692\n",
            "  0.07945752  0.12511318  0.03953736  0.1095591   0.10710955  0.08044268\n",
            "  0.06473993  0.09208608  0.08142553  0.09176706  0.04490552  0.06166664\n",
            "  0.12305601  0.05960573  0.03809519  0.0637179   0.06508007  0.01930667\n",
            "  0.05718909  0.06643799  0.09998113  0.03228119  0.13092916  0.04205034\n",
            "  0.05822642  0.1030962   0.10772337 -0.05159958  0.06508007  0.0637179\n",
            " -0.06912125  0.04526116  0.04097503  0.05788091  0.01020244  0.11320483\n",
            "  0.09589497  0.09589497  0.06677681  0.04383692  0.06677681  0.07149253\n",
            "  0.03809519  0.085658    0.08468509  0.02416727  0.04738919  0.05475919\n",
            "  0.11047374  0.09112827  0.09144779  0.06508007  0.09431227  0.04985932\n",
            "  0.05684278  0.04348016  0.05960573  0.0477429   0.05126476  0.04133375\n",
            "  0.13351683  0.06473993  0.0117317   0.04133375  0.0533647   0.03118295\n",
            " -0.01694414  0.05510714  0.05301539  0.0366485   0.05926131  0.04025676\n",
            "  0.06914112  0.05441098  0.07283043  0.05056259  0.05441098 -0.00417223\n",
            "  0.10371629  0.08338432  0.08436028  0.07978616  0.10371629  0.05056259\n",
            "  0.03845616  0.05822642  0.02639402  0.0370106   0.09272338  0.11561639\n",
            "  0.10833624  0.05545481 -0.08536612  0.07449693  0.0837099   0.08011455\n",
            "  0.03773394  0.05475919  0.01817796  0.04276581  0.09589497  0.07714982\n",
            "  0.08856308  0.05891661  0.06473993  0.03410589  0.02713395  0.09176706\n",
            "  0.12098755  0.04025676  0.08759699  0.09176706  0.02971461  0.09526263\n",
            "  0.05614934  0.11290233  0.10123008  0.06947783  0.05196583  0.05580221\n",
            "  0.07549371  0.08856308  0.10060609  0.07149253  0.08759699  0.002484\n",
            "  0.03592346  0.10371629  0.04490552  0.04419339  0.05649619  0.04419339\n",
            "  0.06439952  0.09208608  0.06132383  0.12128374  0.02491068  0.05021109\n",
            " -0.00220533  0.09621076  0.05161543  0.03592346  0.04526116  0.10587901\n",
            "  0.0859818   0.07316425  0.13580174  0.08142553  0.05614934  0.05753514\n",
            "  0.06914112  0.08011455  0.04276581  0.08436028  0.06779167  0.08044268\n",
            "  0.00287279  0.02787272  0.13351683  0.03592346  0.06405884  0.08142553\n",
            "  0.05960573  0.0837099   0.13551691  0.0901682   0.08011455  0.06098074\n",
            "  0.01817796  0.05475919  0.12039448  0.07615693  0.06880415 -0.00063733\n",
            " -0.00181287  0.02155616  0.05994989  0.11831142  0.0711574   0.02118198\n",
            "  0.08273241  0.0170466   0.0445496   0.02861034  0.04950727  0.06098074\n",
            "  0.08175263  0.09367746  0.11501491  0.05994989  0.08920588  0.05753514\n",
            "  0.02193005  0.05545481  0.09841444  0.03008213  0.04025676  0.05441098\n",
            "  0.09494609  0.04133375  0.03410589  0.06643799  0.02602362  0.04240821\n",
            "  0.09494609  0.05994989  0.00597224  0.07748026]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build a random cut forest with only a small sample of initial points\n",
        "stream_init = 300\n",
        "forest_stream = RobustRandomCutForest(max_samples=128, random_features=False).fit(X[:stream_init])"
      ],
      "metadata": {
        "id": "pcO8qBNTRx-O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now stream in the remaining points\n",
        "for i in range(stream_init, n):\n",
        "    forest_stream.add_point(X[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9lc_CW8RzG-",
        "outputId": "2d7fc307-1461-4a14-cfec-418f94b6c124"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_stream = forest_stream.decision_function(X)"
      ],
      "metadata": {
        "id": "Y1dHkA7jSBpW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_stream"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf9Y_aPASQOS",
        "outputId": "311e0b63-f135-4976-f6a9-772022562def"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04703521,  0.09112827,  0.0151551 , -0.04134659,  0.07781045,\n",
              "        0.02491068,  0.05580221, -0.0113171 ,  0.05926131,  0.06235146,\n",
              "        0.00597224,  0.04240821,  0.05891661,  0.0170466 ,  0.07549371,\n",
              "        0.01020244,  0.06981427,  0.02897872, -0.0057513 ,  0.02005767,\n",
              "        0.04915495,  0.04025676, -0.1030168 ,  0.00828413,  0.05788091,\n",
              "       -0.09552097,  0.02565293,  0.06609891, -0.02632333,  0.0398972 ,\n",
              "        0.04632643,  0.04880235,  0.07582545, -0.0077321 ,  0.04169218,\n",
              "        0.06846692,  0.00789957,  0.00597224, -0.00102887,  0.06063739,\n",
              "        0.04348016, -0.06116866,  0.02971461,  0.07216201,  0.04597161,\n",
              "        0.04597161,  0.06063739,  0.02416727,  0.02005767, -0.01613651,\n",
              "       -0.00812919,  0.09240486, -0.00496115,  0.01211327,  0.06132383,\n",
              "       -0.08308246,  0.08109817,  0.02787272,  0.03737241, -0.00812919,\n",
              "        0.06303522,  0.06981427, -0.01091752,  0.04880235, -0.07628478,\n",
              "        0.03737241,  0.00674407,  0.04240821,  0.04169218, -0.01331969,\n",
              "        0.03483377,  0.03044935,  0.02676413,  0.01401664,  0.04985932,\n",
              "        0.02676413,  0.0477429 ,  0.02416727,  0.07283043,  0.05926131,\n",
              "        0.05753514,  0.06914112,  0.04025676,  0.04950727,  0.00712954,\n",
              "       -0.08262679,  0.03154932,  0.03008213, -0.07493478,  0.08142553,\n",
              "        0.05301539,  0.05684278,  0.00442494,  0.02267696,  0.05196583,\n",
              "        0.06541995,  0.05196583,  0.03881684,  0.02118198,  0.04025676,\n",
              "        0.03228119,  0.01893073,  0.03773394,  0.02379513,  0.05091381,\n",
              "       -0.04092354, -0.0121172 ,  0.0208075 ,  0.07216201,  0.05891661,\n",
              "        0.00519919,  0.06337669,  0.06029378,  0.00674407, -0.08445158,\n",
              "        0.0151551 ,  0.0366485 ,  0.00442494,  0.02043274,  0.04276581,\n",
              "        0.03773394,  0.04276581,  0.07847005,  0.01666889,  0.03519728,\n",
              "        0.02861034, -0.02673496, -0.06956635,  0.04025676,  0.0445496 ,\n",
              "        0.02934681,  0.05960573,  0.04383692,  0.11621692,  0.06812943,\n",
              "        0.02267696,  0.03953736, -0.01573316,  0.06609891,  0.07216201,\n",
              "        0.00558586,  0.04312312,  0.04880235,  0.0075147 ,  0.03374152,\n",
              "        0.05196583,  0.00943601, -0.01613651,  0.07349781,  0.05857165,\n",
              "        0.04419339,  0.04169218,  0.06098074, -0.03671121,  0.02787272,\n",
              "        0.01401664, -0.01533013,  0.09747148,  0.01477591, -0.02263312,\n",
              "        0.05788091,  0.00866839,  0.04668096,  0.05718909,  0.03628612,\n",
              "        0.09176706,  0.05649619,  0.01020244,  0.04880235,  0.07316425,\n",
              "       -0.04219368,  0.03953736, -0.06867649,  0.0326467 ,  0.03301193,\n",
              "        0.03118295,  0.05891661,  0.00287279,  0.06880415,  0.03881684,\n",
              "        0.00014484,  0.05091381, -0.00377824,  0.04561653, -0.00932232,\n",
              "       -0.00024609,  0.01287551, -0.03211552,  0.02304997,  0.02379513,\n",
              "       -0.00063733,  0.09557892,  0.01742402,  0.07482945,  0.02565293,\n",
              "        0.08338432,  0.06200918,  0.04383692,  0.06200918,  0.06914112,\n",
              "        0.04809632,  0.05960573,  0.04950727,  0.04668096,  0.00981938,\n",
              "        0.0908085 , -0.0507378 ,  0.09589497,  0.05788091,  0.0170466 ,\n",
              "        0.00481222,  0.00981938,  0.01629089,  0.05926131, -0.01978085,\n",
              "       -0.06734432, -0.02263312,  0.04526116,  0.01666889,  0.05126476,\n",
              "        0.05441098,  0.03044935,  0.04632643,  0.0477429 ,  0.04348016,\n",
              "        0.0308163 , -0.0443172 ,  0.00053546, -0.05203098,  0.02528195,\n",
              "        0.04169218,  0.07082201,  0.04419339,  0.04561653,  0.06745365,\n",
              "        0.04668096,  0.05196583,  0.0398972 ,  0.0533647 ,  0.05960573,\n",
              "        0.05266581,  0.08500965, -0.01896878, -0.00496115,  0.05196583,\n",
              "        0.04205034,  0.02043274,  0.04240821, -0.02714691,  0.04703521,\n",
              "        0.04950727, -0.00693885,  0.04597161,  0.02304997,  0.02453912,\n",
              "       -0.0780897 ,  0.05753514,  0.07015045,  0.00674407, -0.00535607,\n",
              "        0.01325618,  0.0637179 ,  0.00905235,  0.02639402,  0.01893073,\n",
              "        0.05126476,  0.02491068,  0.03410589, -0.0077321 ,  0.04169218,\n",
              "        0.02824168,  0.09872827,  0.01020244,  0.06981427,  0.00519919,\n",
              "        0.05753514, -0.05159958,  0.08436028,  0.0366485 , -0.02345092,\n",
              "       -0.09227092, -0.00220533,  0.0445496 ,  0.02528195,  0.03483377,\n",
              "        0.04061603,  0.0170466 ,  0.04383692,  0.03845616, -0.0283847 ,\n",
              "        0.04169218,  0.03374152,  0.05441098,  0.03008213,  0.002484  ,\n",
              "        0.02193005, -0.09134559,  0.02787272,  0.0370106 ,  0.02897872,\n",
              "        0.03809519,  0.08791927,  0.00981938,  0.04383692, -0.02632333,\n",
              "        0.03519728, -0.01654017,  0.0117317 ,  0.00403736, -0.02345092,\n",
              "        0.02639402,  0.06098074,  0.07316425,  0.04526116,  0.07048636,\n",
              "        0.03773394,  0.04240821,  0.0063583 ,  0.04844947,  0.03773394,\n",
              "        0.00131579,  0.0533647 ,  0.01666889,  0.04915495, -0.06248633,\n",
              "        0.03410589,  0.05891661,  0.06132383, -0.01412292,  0.07449693,\n",
              "       -0.00693885,  0.06779167,  0.04880235,  0.03556051,  0.0398972 ,\n",
              "        0.00481222,  0.02230365,  0.01591259,  0.02005767,  0.05126476,\n",
              "        0.03228119,  0.06609891,  0.04632643,  0.03953736,  0.02453912,\n",
              "       -0.00142071,  0.12511318,  0.05994989,  0.03917724,  0.03410589,\n",
              "        0.03737241,  0.08207948,  0.0398972 ,  0.05994989,  0.04738919,\n",
              "        0.09048847, -0.0292115 ,  0.00674407,  0.03845616, -0.02345092,\n",
              "        0.01855449,  0.0319154 ,  0.0020949 , -0.06778803, -0.01251771,\n",
              "        0.09684162,  0.07648815,  0.02861034,  0.04348016,  0.06029378,\n",
              "        0.0308163 ,  0.05614934,  0.01249454,  0.05126476,  0.03154932,\n",
              "        0.02528195,  0.01553399,  0.07847005,  0.02934681,  0.04348016,\n",
              "        0.07015045,  0.04205034,  0.04312312,  0.00014484,  0.04880235,\n",
              "       -0.00456654,  0.00905235, -0.00417223, -0.06029192, -0.09319769,\n",
              "        0.02267696, -0.00535607,  0.02713395,  0.05994989,  0.0398972 ,\n",
              "        0.0151551 ,  0.04312312,  0.07615693,  0.05056259,  0.03410589,\n",
              "        0.07714982, -0.07358795, -0.03965638,  0.00828413,  0.06166664,\n",
              "       -0.04092354,  0.05441098,  0.002484  ,  0.05021109, -0.0077321 ,\n",
              "        0.01553399,  0.03410589,  0.00481222,  0.00326128,  0.01780114,\n",
              "        0.02379513,  0.06846692,  0.02602362,  0.05126476,  0.04985932,\n",
              "       -0.09459057,  0.02750348,  0.01287551, -0.06912125,  0.05960573,\n",
              "        0.04025676,  0.00712954,  0.07349781, -0.00417223,  0.02304997,\n",
              "        0.02155616,  0.02602362, -0.07269181,  0.0326467 ,  0.07383111,\n",
              "       -0.10019487, -0.06468933,  0.02193005,  0.02602362,  0.04169218,\n",
              "        0.03374152,  0.02897872,  0.03118295, -0.00024609,  0.03044935,\n",
              "       -0.10632582,  0.01477591,  0.00597224,  0.06029378,  0.02453912,\n",
              "        0.05371373,  0.02528195,  0.03773394,  0.04419339,  0.02565293,\n",
              "        0.04880235, -0.0057513 ,  0.05960573, -0.03545392, -0.00456654,\n",
              "        0.04061603,  0.02639402,  0.00442494,  0.0326467 ,  0.04061603,\n",
              "        0.00828413, -0.01654017,  0.03809519,  0.06677681,  0.0366485 ,\n",
              "        0.00943601,  0.02230365,  0.08044268,  0.05231596, -0.06513096,\n",
              "       -0.00614684,  0.05960573,  0.04915495, -0.05766994,  0.04205034,\n",
              "        0.0151551 ,  0.05441098,  0.03556051,  0.04703521,  0.02118198,\n",
              "       -0.05289478,  0.03953736,  0.0398972 ,  0.01666889,  0.07082201,\n",
              "        0.01591259,  0.04025676,  0.08142553,  0.06235146,  0.06405884,\n",
              "        0.085658  ,  0.0117317 ,  0.05649619, -0.00852659,  0.0063583 ,\n",
              "        0.06880415, -0.07224427,  0.05301539,  0.02676413,  0.04632643,\n",
              "        0.0234227 ,  0.06473993,  0.06643799,  0.02565293,  0.04205034,\n",
              "        0.0117317 ,  0.03592346,  0.03628612,  0.00053546,  0.03118295,\n",
              "        0.02193005,  0.06439952, -0.01452501,  0.03446997,  0.03737241,\n",
              "       -0.01654017, -0.02345092,  0.08759699,  0.04738919,  0.04169218,\n",
              "        0.03446997,  0.07316425, -0.01533013,  0.03519728,  0.08175263,\n",
              "        0.03628612,  0.07015045,  0.07416415,  0.06439952,  0.02676413,\n",
              "        0.0477429 ,  0.05753514,  0.05718909,  0.0234227 ,  0.03337686,\n",
              "        0.0326467 ,  0.0151551 ,  0.00326128,  0.08338432,  0.01096767,\n",
              "        0.04348016, -0.03169969,  0.01780114,  0.0326467 ,  0.04025676,\n",
              "        0.00558586, -0.00812919,  0.00943601,  0.0326467 ,  0.00014484,\n",
              "        0.03483377,  0.01249454,  0.02639402, -0.01937465,  0.03809519,\n",
              "        0.06473993, -0.03461736, -0.01051825,  0.03737241, -0.00812919,\n",
              "        0.06063739,  0.06981427,  0.04133375,  0.06405884,  0.01401664,\n",
              "        0.03008213, -0.0292115 ,  0.02118198,  0.07681911,  0.07879946,\n",
              "        0.02453912,  0.02416727,  0.00597224,  0.02379513,  0.05441098,\n",
              "        0.02934681,  0.0366485 , -0.03965638,  0.07015045, -0.00812919,\n",
              "        0.02005767,  0.04632643,  0.02416727, -0.00535607,  0.04738919,\n",
              "       -0.06424804,  0.05891661,  0.04383692,  0.00943601, -0.03378211,\n",
              "        0.07582545,  0.01325618,  0.02565293,  0.03118295,  0.08984768,\n",
              "        0.06745365,  0.03917724,  0.02934681,  0.00866839,  0.01439642,\n",
              "        0.00287279, -0.00063733,  0.02602362,  0.04526116, -0.00932232,\n",
              "        0.05231596,  0.04844947, -0.00932232,  0.03519728, -0.06160754,\n",
              "       -0.01533013,  0.01591259, -0.04176997,  0.05926131,  0.0533647 ,\n",
              "        0.02934681, -0.01856322,  0.00403736,  0.06473993, -0.06029192,\n",
              "       -0.00338455,  0.0234227 ,  0.10340637,  0.04597161,  0.04312312,\n",
              "        0.06643799, -0.02181659,  0.03228119,  0.05926131,  0.02491068,\n",
              "        0.05684278,  0.02750348,  0.0151551 , -0.08719949,  0.08338432,\n",
              "        0.00981938,  0.00943601,  0.06098074, -0.01613651,  0.04703521,\n",
              "        0.03410589,  0.06981427,  0.06880415, -0.01978085,  0.00131579,\n",
              "        0.00519919,  0.04738919,  0.06745365, -0.03128418,  0.03301193,\n",
              "        0.03301193,  0.06981427,  0.08011455, -0.00852659,  0.02787272,\n",
              "       -0.01412292,  0.05788091,  0.06303522, -0.09459057,  0.00905235,\n",
              "        0.04025676,  0.0370106 ,  0.002484  , -0.01091752,  0.07149253,\n",
              "        0.05994989,  0.02043274,  0.04097503, -0.03461736,  0.06880415,\n",
              "        0.0445496 ,  0.04312312,  0.06541995, -0.01412292,  0.08207948,\n",
              "        0.04025676,  0.0308163 , -0.06424804,  0.03374152,  0.02416727,\n",
              "        0.02379513,  0.04061603,  0.00326128, -0.01896878,  0.04419339,\n",
              "        0.05441098,  0.02676413,  0.02304997,  0.06609891,  0.04025676,\n",
              "        0.02155616,  0.03628612,  0.05161543,  0.05196583,  0.06235146,\n",
              "        0.01096767,  0.01249454,  0.09526263,  0.02005767, -0.03923466,\n",
              "        0.0398972 ,  0.03845616,  0.04276581, -0.04346679,  0.02155616,\n",
              "        0.04025676,  0.07048636, -0.02018736,  0.0319154 ,  0.04880235,\n",
              "        0.05056259,  0.04061603,  0.03410589,  0.04880235,  0.05822642,\n",
              "        0.03592346, -0.03923466,  0.02528195,  0.04597161,  0.0234227 ,\n",
              "        0.01363656,  0.00558586, -0.03419957,  0.08888461,  0.02267696,\n",
              "        0.04061603,  0.01893073,  0.02971461,  0.0170466 ,  0.04809632,\n",
              "        0.04915495,  0.07516171,  0.0637179 ,  0.02934681,  0.02713395,\n",
              "        0.0170466 ,  0.02267696, -0.10585199,  0.02491068,  0.03154932,\n",
              "        0.02416727, -0.00614684,  0.0398972 ,  0.04703521,  0.01780114,\n",
              "        0.03953736,  0.0533647 ,  0.02118198,  0.02528195,  0.0234227 ,\n",
              "        0.0151551 ,  0.03592346,  0.0895269 ,  0.03773394,  0.0445496 ,\n",
              "        0.0718274 ,  0.08533395,  0.02713395,  0.05545481,  0.05406249,\n",
              "       -0.07090374,  0.02005767,  0.01893073,  0.04844947,  0.02304997,\n",
              "        0.04880235, -0.03336497,  0.02230365,  0.00287279,  0.05371373,\n",
              "        0.01325618,  0.07449693,  0.0170466 ,  0.07015045, -0.03294816,\n",
              "        0.06235146,  0.06812943,  0.08240607, -0.02962539, -0.02059419,\n",
              "        0.04419339,  0.002484  ,  0.07283043,  0.04526116,  0.06063739,\n",
              "        0.04950727, -0.0089243 ,  0.04169218,  0.10741658,  0.0117317 ,\n",
              "        0.05091381,  0.06643799,  0.04276581,  0.03154932,  0.07482945,\n",
              "        0.04561653,  0.0445496 ,  0.03556051,  0.01780114,  0.08011455,\n",
              "        0.02565293,  0.00519919,  0.0319154 ,  0.03519728,  0.03008213,\n",
              "        0.05301539,  0.02934681,  0.09208608,  0.02787272,  0.08856308,\n",
              "        0.04025676,  0.03228119,  0.06981427,  0.06303522, -0.04007844,\n",
              "        0.02713395,  0.04597161, -0.07001179,  0.0105852 ,  0.01553399,\n",
              "        0.0308163 , -0.00417223,  0.07449693,  0.06337669,  0.04240821,\n",
              "        0.03556051, -0.00181287,  0.03845616,  0.06711536,  0.01363656,\n",
              "        0.05301539,  0.02639402, -0.02181659,  0.03483377, -0.00456654,\n",
              "        0.1101691 ,  0.09240486,  0.05126476,  0.03556051,  0.06575956,\n",
              "        0.01930667,  0.02897872,  0.0319154 ,  0.04097503,  0.06711536,\n",
              "        0.02750348,  0.01855449,  0.09715667,  0.03737241,  0.01020244,\n",
              "        0.02043274,  0.0105852 ,  0.02750348, -0.04092354,  0.04097503,\n",
              "        0.02193005,  0.01893073,  0.04133375,  0.02155616,  0.0445496 ,\n",
              "        0.03446997,  0.0308163 ,  0.01817796,  0.03483377, -0.04261772,\n",
              "        0.05614934,  0.05231596,  0.04383692,  0.05857165,  0.06508007,\n",
              "        0.02267696, -0.00259809,  0.00828413,  0.00866839,  0.02676413,\n",
              "        0.05126476,  0.07847005,  0.07416415,  0.0308163 , -0.10254556,\n",
              "        0.04985932,  0.05441098,  0.06914112, -0.00063733,  0.03953736,\n",
              "        0.02602362,  0.02602362,  0.04276581,  0.02861034,  0.04632643,\n",
              "        0.03737241,  0.02379513,  0.0445496 ,  0.02379513,  0.0398972 ,\n",
              "        0.08920588,  0.02453912,  0.05857165,  0.06132383, -0.00693885,\n",
              "        0.07316425,  0.04915495,  0.07283043,  0.06439952,  0.0326467 ,\n",
              "        0.02230365,  0.02230365,  0.03881684,  0.03228119,  0.06166664,\n",
              "        0.04490552,  0.05266581, -0.02018736,  0.03044935,  0.09494609,\n",
              "        0.03301193,  0.04205034, -0.00220533,  0.03301193, -0.00024609,\n",
              "        0.04561653,  0.01666889,  0.10772337,  0.02491068,  0.02416727,\n",
              "       -0.04261772,  0.05960573,  0.02043274,  0.02934681,  0.02565293,\n",
              "        0.06508007,  0.04809632,  0.04133375,  0.1030962 ,  0.06269347,\n",
              "        0.02193005,  0.02602362,  0.05545481,  0.05718909,  0.04061603,\n",
              "        0.02453912,  0.05753514,  0.06541995, -0.02059419, -0.01573316,\n",
              "        0.09872827,  0.02453912,  0.02750348,  0.04526116,  0.0445496 ,\n",
              "        0.05718909,  0.09144779,  0.07149253,  0.04915495,  0.04597161,\n",
              "        0.0170466 ,  0.02304997,  0.09462931,  0.02565293,  0.0308163 ,\n",
              "       -0.01171699, -0.01291855, -0.00812919,  0.03410589,  0.08142553,\n",
              "        0.06405884,  0.02565293,  0.08403521,  0.02824168,  0.03154932,\n",
              "       -0.00063733,  0.02750348,  0.04668096,  0.05580221,  0.05126476,\n",
              "        0.10123008,  0.01968232,  0.06029378,  0.04526116,  0.01439642,\n",
              "        0.04668096,  0.06063739,  0.01134983, -0.00024609,  0.0308163 ,\n",
              "        0.07249635,  0.00481222, -0.00338455, -0.01654017,  0.01134983,\n",
              "        0.02005767,  0.04419339,  0.04133375, -0.0238603 ,  0.05684278])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# both random cut forests produced good results at identifying the anomalies\n",
        "print('batch random cut forest roc auc: ', metrics.roc_auc_score(is_outlier, -scores_batch))\n",
        "print('streaming random cut forest roc auc: ', metrics.roc_auc_score(is_outlier, -scores_stream))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNC296k9SDaR",
        "outputId": "5a8d93ab-11ec-47dd-dd6d-8d405f448b9a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch random cut forest roc auc:  0.9985473785473785\n",
            "streaming random cut forest roc auc:  0.9935257335257335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Approach\n"
      ],
      "metadata": {
        "id": "96zwlLmjTE2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest = RobustRandomCutForest()\n",
        "forest = forest.fit(X)"
      ],
      "metadata": {
        "id": "p5icIe-mMPwz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depths = forest.decision_function(X)\n",
        "labels = forest.predict(X)"
      ],
      "metadata": {
        "id": "jOCjD4C2RJOW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Depths: ', depths)\n",
        "print('Labels: ', labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdCkJWvNTTHP",
        "outputId": "d1cb3566-4143-47f3-9fe0-147a321ec47a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depths:  [ 0.12494216  0.17371971  0.05794785 -0.00368904  0.14107302  0.12111786\n",
            "  0.10914516  0.06799691  0.13593885  0.1456558   0.06974641  0.09842699\n",
            "  0.12620832  0.06181745  0.1344585   0.05132128  0.1236717   0.06682663\n",
            "  0.07667386  0.05764877  0.08461503  0.10993739 -0.0593581   0.05192778\n",
            "  0.09761138 -0.06697569  0.10409017  0.11151704  0.02765579  0.08685645\n",
            "  0.11387454  0.10940941  0.11204217  0.03494651  0.11491771  0.14445551\n",
            "  0.05645045  0.07466496  0.05374236  0.12822531  0.07895813 -0.02525897\n",
            "  0.09102657  0.14541607  0.11725457  0.08236145  0.12948039  0.08123007\n",
            "  0.05010582  0.02637622  0.03651649  0.1499434   0.03431702  0.04827646\n",
            "  0.12922971 -0.05258981  0.14373338  0.09102657  0.08377136  0.05132128\n",
            "  0.12290736  0.12747022  0.06594683  0.09788344 -0.05371215  0.11335189\n",
            "  0.05794785  0.07495252  0.06033316  0.05944018  0.09350841  0.09460661\n",
            "  0.10435785  0.05374236  0.11803037  0.06799691  0.12948039  0.05824672\n",
            "  0.14155819  0.11751334  0.13667677  0.1398572   0.10221138  0.12034833\n",
            "  0.09706672 -0.04039328  0.07696007  0.09788344 -0.02101325  0.15627772\n",
            "  0.11931987  0.10059388  0.06122434  0.1136133   0.10355428  0.11699562\n",
            "  0.12747022  0.09378323  0.08908578  0.10569351  0.08320797  0.07466496\n",
            "  0.11517807  0.10993739  0.10755586 -0.00029395  0.05404407  0.08908578\n",
            "  0.14300979  0.11699562  0.07235739  0.12163001  0.13123039  0.0573495\n",
            " -0.07625386  0.05824672  0.12009148  0.0405737   0.06181745  0.13667677\n",
            "  0.13297213  0.07753191  0.13322028  0.08207889  0.12341709  0.05824672\n",
            "  0.00374997 -0.00985838  0.08629723  0.1117797   0.08179614  0.12570237\n",
            "  0.11621772  0.18711852  0.10005326  0.09542831  0.10914516  0.04888707\n",
            "  0.12721818  0.14107302  0.03588913  0.06799691  0.12443449  0.08348976\n",
            "  0.07466496  0.13346826  0.05132128  0.04459539  0.14804424  0.12494216\n",
            "  0.08461503  0.09405788  0.13147972  0.05313832  0.09213147  0.1164772\n",
            "  0.04551846  0.14469589  0.09405788  0.02541427  0.12519573  0.0752399\n",
            "  0.13147972  0.13421119  0.07952727  0.16431858  0.14180054  0.05884385\n",
            "  0.13814814  0.15790116 -0.00573715  0.07724608 -0.0238399   0.06359197\n",
            "  0.12519573  0.08685645  0.1136133   0.04490328  0.14493611  0.11125421\n",
            "  0.05645045  0.0995119   0.03682985  0.08094675  0.04980145  0.06063042\n",
            "  0.03870557  0.01174001  0.07206807  0.08264381  0.06653356  0.16431858\n",
            "  0.09815531  0.12009148  0.07148883  0.15488007  0.1117797   0.10435785\n",
            "  0.13123039  0.1401007   0.11957725  0.11491771  0.1241804   0.10993739\n",
            "  0.04582573  0.16431858 -0.02030898  0.17438106  0.11621772  0.09542831\n",
            "  0.0621137   0.06388703  0.07177855  0.14517617  0.03557514 -0.04442858\n",
            "  0.06270561  0.11491771  0.04705275  0.10755586  0.11854669  0.08264381\n",
            "  0.08489587  0.08573725  0.1117797   0.08545698 -0.0126245   0.02701644\n",
            " -0.01540564  0.0662403   0.12316231  0.13172887  0.10248032  0.12922971\n",
            "  0.15371103  0.07838822  0.13073124  0.09102657  0.09460661  0.09761138\n",
            "  0.1136133   0.15604517  0.00942296  0.07003731  0.08769386  0.10993739\n",
            "  0.10967349  0.09761138  0.02765579  0.10861611  0.10274908  0.04551846\n",
            "  0.13643096  0.05615037  0.0573495  -0.01925435  0.11751334  0.12595543\n",
            "  0.07119892  0.03995183  0.04551846  0.08657693  0.03400196  0.10140345\n",
            "  0.07119892  0.1241804   0.07838822  0.10622651  0.05101773  0.0995119\n",
            "  0.07781754  0.16725705  0.09706672  0.12034833  0.07322419  0.12086152\n",
            "  0.01503111  0.13961355  0.11335189  0.03242346 -0.07742425  0.04026287\n",
            "  0.10861611  0.10032366  0.08880777  0.07895813  0.07293545  0.08852957\n",
            "  0.09842699  0.04274361  0.08292599  0.0752399   0.10622651  0.08405278\n",
            "  0.01535899  0.08991867 -0.05558777  0.09706672  0.09433233  0.11777194\n",
            "  0.09213147  0.14661312  0.04827646  0.12948039  0.02573514  0.1059601\n",
            "  0.07235739  0.07552708  0.06152099  0.01107912  0.02251668  0.13371607\n",
            "  0.14756785  0.10967349  0.14899511  0.07466496  0.10005326  0.0573495\n",
            "  0.11983445  0.08825119  0.04674631  0.10993739  0.08066324  0.08852957\n",
            " -0.01123957  0.1130903   0.12620832  0.10113378  0.05404407  0.13396371\n",
            "  0.00875893  0.12265224  0.13839279  0.09896982  0.05914212  0.07437719\n",
            "  0.05253346  0.07466496  0.07322419  0.07867327  0.06300126  0.12111786\n",
            "  0.05944018  0.10221138  0.09488069  0.01634132  0.18070536  0.14083018\n",
            "  0.11204217  0.08405278  0.10702466  0.16249742  0.09924095  0.14131569\n",
            "  0.09597518  0.14875763  0.01140968  0.10032366  0.06240976  0.02089928\n",
            "  0.05794785  0.10355428  0.07322419 -0.00779358  0.03242346  0.16703193\n",
            "  0.14083018  0.06535931  0.10382232  0.1236717   0.06152099  0.09815531\n",
            "  0.06770464  0.09295819  0.0662403   0.09570184  0.05554959  0.12897887\n",
            "  0.08292599  0.07437719  0.13470564  0.10382232  0.12111786  0.05705002\n",
            "  0.10113378  0.06447654  0.06565317  0.01240001 -0.00745026 -0.07005169\n",
            "  0.08991867  0.04367044  0.09515459  0.14058719  0.12214147  0.04766502\n",
            "  0.08264381  0.13322028  0.11335189  0.06799691  0.14373338 -0.07005169\n",
            "  0.00105767  0.04551846  0.10032366 -0.00029395  0.12570237 -0.00131007\n",
            "  0.0986985   0.02348451  0.09788344  0.10675879  0.06388703  0.04980145\n",
            "  0.0671195   0.05973804  0.12671359  0.05884385  0.12214147  0.12646104\n",
            " -0.07859703  0.09378323  0.08264381 -0.0316925   0.12060502  0.11230447\n",
            "  0.07667386  0.14131569  0.05404407  0.04919207  0.08852957  0.09570184\n",
            " -0.03929794  0.09924095  0.14445551 -0.08173608 -0.01855245  0.07177855\n",
            "  0.09761138  0.11020111  0.08377136  0.04428728  0.11725457  0.09019592\n",
            "  0.07148883 -0.06928113  0.04949686  0.05374236  0.11413559  0.08377136\n",
            "  0.11880459  0.11282853  0.1130903   0.07667386  0.1344585   0.09515459\n",
            "  0.04643966  0.09515459 -0.01089392  0.06240976  0.10248032  0.08545698\n",
            "  0.07466496  0.08517652  0.10382232  0.03776866  0.00975464  0.10569351\n",
            "  0.14083018  0.10274908  0.07696007  0.08685645  0.14204272  0.13790333\n",
            " -0.02775159  0.04980145  0.13147972  0.08741491 -0.02703821  0.08517652\n",
            "  0.07895813  0.13123039  0.06506525  0.08991867  0.09652132 -0.01540564\n",
            "  0.11072802  0.08489587  0.09350841  0.13172887  0.06359197  0.10835131\n",
            "  0.13495262  0.10301766  0.12897887  0.12570237  0.07061851  0.09433233\n",
            "  0.02251668  0.07235739  0.1309809  -0.02775159  0.11621772  0.08292599\n",
            "  0.11699562  0.06152099  0.12188583  0.13197786  0.09378323  0.11803037\n",
            "  0.06418188  0.09652132  0.10409017  0.04705275  0.11072802  0.05615037\n",
            "  0.11906232  0.07667386  0.09295819  0.12392613  0.05041     0.01699511\n",
            "  0.12468841  0.11880459  0.04212469  0.07408923  0.12822531  0.02541427\n",
            "  0.06594683  0.11957725  0.07952727  0.15441293  0.14058719  0.13569254\n",
            "  0.07061851  0.11491771  0.09295819  0.11906232  0.07351273  0.06181745\n",
            "  0.0752399   0.04336171  0.04181491  0.12998124  0.06152099  0.08489587\n",
            "  0.02348451  0.07090881  0.0461328   0.08880777  0.0600357   0.04428728\n",
            "  0.04980145  0.08797262  0.064771    0.05824672  0.04088431  0.08123007\n",
            " -0.00505352  0.10569351  0.11204217 -0.03602523  0.04088431  0.12214147\n",
            "  0.06033316  0.11828862  0.11854669  0.12392613  0.1109912   0.04797084\n",
            "  0.05132128 -0.01401319  0.06682663  0.1555796   0.14899511  0.08657693\n",
            "  0.05944018  0.07090881  0.07696007  0.09157939  0.07206807  0.10328606\n",
            "  0.01272968  0.10861611  0.03494651  0.04212469  0.13912574  0.05764877\n",
            "  0.02316212  0.08320797 -0.02882348  0.14373338  0.08037954  0.05794785\n",
            " -0.00029395  0.13667677  0.03870557  0.03179056  0.07753191  0.16680665\n",
            "  0.12468841  0.07293545  0.06770464  0.05374236  0.07696007  0.09706672\n",
            "  0.03147379  0.07119892  0.10782119  0.03932912  0.10967349  0.10489265\n",
            "  0.05854539  0.07495252 -0.03205222  0.05374236  0.08936359  0.02445037\n",
            "  0.12009148  0.10914516  0.08517652 -0.0026681   0.06181745  0.12620832\n",
            " -0.03856895  0.04705275  0.09157939  0.17657588  0.10328606  0.1059601\n",
            "  0.09597518  0.00105767  0.12239694  0.15136104  0.10086392  0.13569254\n",
            "  0.05854539  0.07610085 -0.06277327  0.10274908  0.08123007  0.04949686\n",
            "  0.14517617 -0.00916918  0.07322419  0.08320797  0.12620832  0.15651011\n",
            "  0.04336171  0.02701644  0.01568666  0.10649274  0.1309809   0.00173211\n",
            "  0.1117797   0.06359197  0.10649274  0.14325115  0.04336171  0.08066324\n",
            "  0.03463187  0.10301766  0.14252658 -0.06087339  0.05494799  0.08489587\n",
            "  0.07753191  0.05884385 -0.01123957  0.12544914  0.08377136  0.07867327\n",
            "  0.07003731  0.014703    0.13716789  0.10059388  0.09378323  0.14228473\n",
            "  0.00575966  0.1401007   0.10355428  0.04521097 -0.0126245   0.10835131\n",
            "  0.08741491  0.08348976  0.08461503  0.07810297  0.05192778  0.10835131\n",
            "  0.08825119  0.09542831  0.0662403   0.14276827  0.05283599  0.08685645\n",
            "  0.0926828   0.09460661  0.11854669  0.12163001  0.11491771  0.05101773\n",
            "  0.13692242  0.07581406 -0.00710718  0.09542831  0.08685645  0.07322419\n",
            "  0.0196014   0.04367044  0.10435785  0.12646104  0.02541427  0.08629723\n",
            "  0.09433233  0.11517807  0.09130307  0.07003731  0.12922971  0.10382232\n",
            "  0.11072802 -0.02882348  0.09542831  0.10489265  0.08123007  0.07148883\n",
            "  0.03210712  0.00475583  0.15813245  0.04827646  0.12290736  0.04088431\n",
            "  0.07981155  0.07177855  0.064771    0.0995119   0.1574381   0.14445551\n",
            "  0.04521097  0.08545698  0.06565317  0.0461328  -0.04664234  0.06535931\n",
            "  0.10940941  0.07924279 -0.00232825  0.08769386  0.10301766  0.05794785\n",
            "  0.08991867  0.12747022  0.08908578  0.06682663  0.09019592  0.07380108\n",
            "  0.10328606  0.168156    0.12494216  0.06418188  0.15018007  0.13839279\n",
            "  0.08405278  0.13421119  0.10702466 -0.06277327  0.07895813  0.04243425\n",
            "  0.1109912   0.06741217  0.11230447 -0.00710718  0.07351273  0.02797514\n",
            "  0.07952727  0.03808118  0.14107302  0.06506525  0.14058719 -0.03025605\n",
            "  0.09652132  0.12163001  0.15371103 -0.01401319  0.0289319   0.09978267\n",
            "  0.05884385  0.10940941  0.09896982  0.1236717   0.12341709  0.05554959\n",
            "  0.12595543  0.13790333  0.06945532  0.14661312  0.12797378  0.11125421\n",
            "  0.10702466  0.14732941  0.1164772   0.09405788  0.08009564  0.08852957\n",
            "  0.15951692  0.07838822  0.04367044  0.07867327  0.09295819  0.04827646\n",
            "  0.08991867  0.04949686  0.15253802  0.08545698  0.15604517  0.07119892\n",
            "  0.09815531  0.15088913  0.13371607 -0.0231318   0.05615037  0.11621772\n",
            " -0.04112474  0.07032801  0.06741217  0.08009564  0.02445037  0.1164772\n",
            "  0.12265224  0.11491771  0.08179614  0.04181491  0.10086392  0.08320797\n",
            "  0.06945532  0.09185553  0.1117797   0.04119472  0.06887255  0.05464689\n",
            "  0.15813245  0.14732941  0.1109912   0.08573725  0.13790333  0.10808634\n",
            "  0.07351273  0.08825119  0.08797262  0.10967349  0.06300126  0.06506525\n",
            "  0.18859629  0.10409017  0.05344044  0.02445037  0.0662403   0.07293545\n",
            " -0.03313284  0.1109912   0.07895813  0.05494799  0.08066324  0.01830002\n",
            "  0.1164772   0.09019592  0.07264652  0.05705002  0.08094675  0.01074833\n",
            "  0.12671359  0.1109912   0.11125421  0.14349235  0.12544914  0.11256659\n",
            "  0.05615037  0.08405278  0.02187038  0.064771    0.10435785  0.15183231\n",
            "  0.10113378  0.08517652 -0.06774313  0.09019592  0.10435785  0.12721818\n",
            "  0.06770464  0.08037954  0.05585008  0.0752399   0.11046465  0.11413559\n",
            "  0.11517807  0.06447654  0.08236145  0.06033316  0.08545698  0.0986985\n",
            "  0.14732941  0.064771    0.12998124  0.11906232  0.03020456  0.15088913\n",
            "  0.10940941  0.13371607  0.12620832  0.05794785  0.07867327  0.08320797\n",
            "  0.11569825  0.07753191  0.10435785  0.09488069  0.09896982  0.04428728\n",
            "  0.06653356  0.12922971  0.07235739  0.04888707  0.05944018  0.06653356\n",
            "  0.04643966  0.12797378  0.09295819  0.15651011  0.08880777  0.08629723\n",
            " -0.0033485   0.11491771  0.07638745  0.05344044  0.08629723  0.11777194\n",
            "  0.14252658  0.08908578  0.15581247  0.11828862  0.10274908  0.06799691\n",
            "  0.11020111  0.11828862  0.09074987  0.08825119  0.11595807  0.10861611\n",
            "  0.01174001  0.07552708  0.16340923  0.03273959  0.10729035  0.1167365\n",
            "  0.06181745  0.10914516  0.15766971  0.12897887  0.10914516  0.09924095\n",
            "  0.04827646  0.06653356  0.15018007  0.06447654  0.07981155  0.0405737\n",
            " -0.00573715  0.01830002  0.09213147  0.1309809   0.08629723  0.05645045\n",
            "  0.16905253  0.07032801  0.10515979  0.03526093  0.10409017  0.10328606\n",
            "  0.10515979  0.10113378  0.14155819  0.08741491  0.12797378  0.07610085\n",
            "  0.05010582  0.08852957  0.10755586  0.07032801  0.03431702  0.04212469\n",
            "  0.11983445  0.07032801  0.02316212  0.04336171  0.04026287  0.0600357\n",
            "  0.11517807  0.07781754  0.00842658  0.10861611]\n",
            "Labels:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "df = pd.DataFrame(X)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "9aCTb9c0TnYW",
        "outputId": "ac00d97a-e961-4d54-be32-daa7a31c1c3b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0 -2.284842 -0.940595  1.580589 -0.936700  0.201533  0.562792  0.222527   \n",
              "1 -0.112024 -0.909350  1.456383  0.729539 -0.144048 -0.811780 -0.322083   \n",
              "2 -0.316043 -0.338892  0.243533  0.565637  1.499923  1.597672  1.665715   \n",
              "3  1.243562  0.953326  0.916870  0.242212  2.861265  0.855922  1.382748   \n",
              "4 -0.382760 -1.591869  1.212411 -0.544538  0.523952 -0.038625  0.746030   \n",
              "\n",
              "         7         8         9         10        11        12        13  \\\n",
              "0 -0.277234 -1.077819 -0.493098 -0.592255 -0.111492 -0.749112  0.273309   \n",
              "1  0.177745 -0.561323 -0.232063 -0.218400 -0.530819  0.111559 -0.339777   \n",
              "2 -0.087915 -1.744642 -0.987217 -1.913087 -0.830700 -1.235151  0.268027   \n",
              "3  1.629768  1.737861  1.821615  0.664029  2.487674  0.674101  0.654034   \n",
              "4  0.006171 -1.043119 -0.210697 -0.557552 -0.072771 -1.511380  0.867469   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0 -1.044840  0.553838  0.287068 -0.325217 -0.847720  0.514126  \n",
              "1 -1.461972  0.264692 -0.127786  0.488363  0.292738  0.046393  \n",
              "2  0.434729 -0.657030 -1.566755 -1.140797 -0.565137  0.711909  \n",
              "3  1.018153  1.082980  2.568560  2.996957  2.049397  0.513600  \n",
              "4  0.565315  0.426926 -0.684476  0.623326 -0.093837 -0.952544  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3909e140-9167-4580-ace4-659f23971bc2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.284842</td>\n",
              "      <td>-0.940595</td>\n",
              "      <td>1.580589</td>\n",
              "      <td>-0.936700</td>\n",
              "      <td>0.201533</td>\n",
              "      <td>0.562792</td>\n",
              "      <td>0.222527</td>\n",
              "      <td>-0.277234</td>\n",
              "      <td>-1.077819</td>\n",
              "      <td>-0.493098</td>\n",
              "      <td>-0.592255</td>\n",
              "      <td>-0.111492</td>\n",
              "      <td>-0.749112</td>\n",
              "      <td>0.273309</td>\n",
              "      <td>-1.044840</td>\n",
              "      <td>0.553838</td>\n",
              "      <td>0.287068</td>\n",
              "      <td>-0.325217</td>\n",
              "      <td>-0.847720</td>\n",
              "      <td>0.514126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.112024</td>\n",
              "      <td>-0.909350</td>\n",
              "      <td>1.456383</td>\n",
              "      <td>0.729539</td>\n",
              "      <td>-0.144048</td>\n",
              "      <td>-0.811780</td>\n",
              "      <td>-0.322083</td>\n",
              "      <td>0.177745</td>\n",
              "      <td>-0.561323</td>\n",
              "      <td>-0.232063</td>\n",
              "      <td>-0.218400</td>\n",
              "      <td>-0.530819</td>\n",
              "      <td>0.111559</td>\n",
              "      <td>-0.339777</td>\n",
              "      <td>-1.461972</td>\n",
              "      <td>0.264692</td>\n",
              "      <td>-0.127786</td>\n",
              "      <td>0.488363</td>\n",
              "      <td>0.292738</td>\n",
              "      <td>0.046393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.316043</td>\n",
              "      <td>-0.338892</td>\n",
              "      <td>0.243533</td>\n",
              "      <td>0.565637</td>\n",
              "      <td>1.499923</td>\n",
              "      <td>1.597672</td>\n",
              "      <td>1.665715</td>\n",
              "      <td>-0.087915</td>\n",
              "      <td>-1.744642</td>\n",
              "      <td>-0.987217</td>\n",
              "      <td>-1.913087</td>\n",
              "      <td>-0.830700</td>\n",
              "      <td>-1.235151</td>\n",
              "      <td>0.268027</td>\n",
              "      <td>0.434729</td>\n",
              "      <td>-0.657030</td>\n",
              "      <td>-1.566755</td>\n",
              "      <td>-1.140797</td>\n",
              "      <td>-0.565137</td>\n",
              "      <td>0.711909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.243562</td>\n",
              "      <td>0.953326</td>\n",
              "      <td>0.916870</td>\n",
              "      <td>0.242212</td>\n",
              "      <td>2.861265</td>\n",
              "      <td>0.855922</td>\n",
              "      <td>1.382748</td>\n",
              "      <td>1.629768</td>\n",
              "      <td>1.737861</td>\n",
              "      <td>1.821615</td>\n",
              "      <td>0.664029</td>\n",
              "      <td>2.487674</td>\n",
              "      <td>0.674101</td>\n",
              "      <td>0.654034</td>\n",
              "      <td>1.018153</td>\n",
              "      <td>1.082980</td>\n",
              "      <td>2.568560</td>\n",
              "      <td>2.996957</td>\n",
              "      <td>2.049397</td>\n",
              "      <td>0.513600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.382760</td>\n",
              "      <td>-1.591869</td>\n",
              "      <td>1.212411</td>\n",
              "      <td>-0.544538</td>\n",
              "      <td>0.523952</td>\n",
              "      <td>-0.038625</td>\n",
              "      <td>0.746030</td>\n",
              "      <td>0.006171</td>\n",
              "      <td>-1.043119</td>\n",
              "      <td>-0.210697</td>\n",
              "      <td>-0.557552</td>\n",
              "      <td>-0.072771</td>\n",
              "      <td>-1.511380</td>\n",
              "      <td>0.867469</td>\n",
              "      <td>0.565315</td>\n",
              "      <td>0.426926</td>\n",
              "      <td>-0.684476</td>\n",
              "      <td>0.623326</td>\n",
              "      <td>-0.093837</td>\n",
              "      <td>-0.952544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3909e140-9167-4580-ace4-659f23971bc2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3909e140-9167-4580-ace4-659f23971bc2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3909e140-9167-4580-ace4-659f23971bc2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['depths'] = depths\n",
        "df['labels'] = labels "
      ],
      "metadata": {
        "id": "qR1LWdJJT5CG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "L2cUbQPtUSju",
        "outputId": "e9d983ed-e843-4121-bc53-0b173542bedd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0 -2.284842 -0.940595  1.580589 -0.936700  0.201533  0.562792  0.222527   \n",
              "1 -0.112024 -0.909350  1.456383  0.729539 -0.144048 -0.811780 -0.322083   \n",
              "2 -0.316043 -0.338892  0.243533  0.565637  1.499923  1.597672  1.665715   \n",
              "3  1.243562  0.953326  0.916870  0.242212  2.861265  0.855922  1.382748   \n",
              "4 -0.382760 -1.591869  1.212411 -0.544538  0.523952 -0.038625  0.746030   \n",
              "\n",
              "          7         8         9  ...        12        13        14        15  \\\n",
              "0 -0.277234 -1.077819 -0.493098  ... -0.749112  0.273309 -1.044840  0.553838   \n",
              "1  0.177745 -0.561323 -0.232063  ...  0.111559 -0.339777 -1.461972  0.264692   \n",
              "2 -0.087915 -1.744642 -0.987217  ... -1.235151  0.268027  0.434729 -0.657030   \n",
              "3  1.629768  1.737861  1.821615  ...  0.674101  0.654034  1.018153  1.082980   \n",
              "4  0.006171 -1.043119 -0.210697  ... -1.511380  0.867469  0.565315  0.426926   \n",
              "\n",
              "         16        17        18        19    depths  labels  \n",
              "0  0.287068 -0.325217 -0.847720  0.514126  0.124942       1  \n",
              "1 -0.127786  0.488363  0.292738  0.046393  0.173720       1  \n",
              "2 -1.566755 -1.140797 -0.565137  0.711909  0.057948       1  \n",
              "3  2.568560  2.996957  2.049397  0.513600 -0.003689       1  \n",
              "4 -0.684476  0.623326 -0.093837 -0.952544  0.141073       1  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6525718f-d3b5-4967-84cd-c73a5e65ab00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>depths</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.284842</td>\n",
              "      <td>-0.940595</td>\n",
              "      <td>1.580589</td>\n",
              "      <td>-0.936700</td>\n",
              "      <td>0.201533</td>\n",
              "      <td>0.562792</td>\n",
              "      <td>0.222527</td>\n",
              "      <td>-0.277234</td>\n",
              "      <td>-1.077819</td>\n",
              "      <td>-0.493098</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.749112</td>\n",
              "      <td>0.273309</td>\n",
              "      <td>-1.044840</td>\n",
              "      <td>0.553838</td>\n",
              "      <td>0.287068</td>\n",
              "      <td>-0.325217</td>\n",
              "      <td>-0.847720</td>\n",
              "      <td>0.514126</td>\n",
              "      <td>0.124942</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.112024</td>\n",
              "      <td>-0.909350</td>\n",
              "      <td>1.456383</td>\n",
              "      <td>0.729539</td>\n",
              "      <td>-0.144048</td>\n",
              "      <td>-0.811780</td>\n",
              "      <td>-0.322083</td>\n",
              "      <td>0.177745</td>\n",
              "      <td>-0.561323</td>\n",
              "      <td>-0.232063</td>\n",
              "      <td>...</td>\n",
              "      <td>0.111559</td>\n",
              "      <td>-0.339777</td>\n",
              "      <td>-1.461972</td>\n",
              "      <td>0.264692</td>\n",
              "      <td>-0.127786</td>\n",
              "      <td>0.488363</td>\n",
              "      <td>0.292738</td>\n",
              "      <td>0.046393</td>\n",
              "      <td>0.173720</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.316043</td>\n",
              "      <td>-0.338892</td>\n",
              "      <td>0.243533</td>\n",
              "      <td>0.565637</td>\n",
              "      <td>1.499923</td>\n",
              "      <td>1.597672</td>\n",
              "      <td>1.665715</td>\n",
              "      <td>-0.087915</td>\n",
              "      <td>-1.744642</td>\n",
              "      <td>-0.987217</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.235151</td>\n",
              "      <td>0.268027</td>\n",
              "      <td>0.434729</td>\n",
              "      <td>-0.657030</td>\n",
              "      <td>-1.566755</td>\n",
              "      <td>-1.140797</td>\n",
              "      <td>-0.565137</td>\n",
              "      <td>0.711909</td>\n",
              "      <td>0.057948</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.243562</td>\n",
              "      <td>0.953326</td>\n",
              "      <td>0.916870</td>\n",
              "      <td>0.242212</td>\n",
              "      <td>2.861265</td>\n",
              "      <td>0.855922</td>\n",
              "      <td>1.382748</td>\n",
              "      <td>1.629768</td>\n",
              "      <td>1.737861</td>\n",
              "      <td>1.821615</td>\n",
              "      <td>...</td>\n",
              "      <td>0.674101</td>\n",
              "      <td>0.654034</td>\n",
              "      <td>1.018153</td>\n",
              "      <td>1.082980</td>\n",
              "      <td>2.568560</td>\n",
              "      <td>2.996957</td>\n",
              "      <td>2.049397</td>\n",
              "      <td>0.513600</td>\n",
              "      <td>-0.003689</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.382760</td>\n",
              "      <td>-1.591869</td>\n",
              "      <td>1.212411</td>\n",
              "      <td>-0.544538</td>\n",
              "      <td>0.523952</td>\n",
              "      <td>-0.038625</td>\n",
              "      <td>0.746030</td>\n",
              "      <td>0.006171</td>\n",
              "      <td>-1.043119</td>\n",
              "      <td>-0.210697</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.511380</td>\n",
              "      <td>0.867469</td>\n",
              "      <td>0.565315</td>\n",
              "      <td>0.426926</td>\n",
              "      <td>-0.684476</td>\n",
              "      <td>0.623326</td>\n",
              "      <td>-0.093837</td>\n",
              "      <td>-0.952544</td>\n",
              "      <td>0.141073</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6525718f-d3b5-4967-84cd-c73a5e65ab00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6525718f-d3b5-4967-84cd-c73a5e65ab00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6525718f-d3b5-4967-84cd-c73a5e65ab00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Given an array of points....\n",
        "# for point in points:\n",
        "#     forest.add_point(point)\n",
        "# depths = forest.decision_function(points)\n",
        "# labels = forest.predict(points)"
      ],
      "metadata": {
        "id": "exCvMA0yTKe3"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}