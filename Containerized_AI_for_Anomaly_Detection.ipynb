{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVNBXYxm3ZtTO3WlkbZxeG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/swlh/containerized-ai-for-anomaly-detection-eb3e08225235)"
      ],
      "metadata": {
        "id": "qswWBzkSgWI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xYr4juoPgS-Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This python file uses the Flask framework to accept sensor data via a REST API for anomaly detection  \n",
        "by an AI neural network. The neural network model has been pre-trained is loaded and executed \n",
        "using Keras and TensorFlow.\n",
        "Usage:\n",
        "Start the server:\n",
        "   python app.py\n",
        "Submit a request via cURL:\n",
        "   curl -X POST -F data_file=@day4_data.csv 'http://localhost:5000/submit'\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import flask\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import csv\n",
        "import codecs\n",
        "\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# initialize the Flask application\n",
        "app = flask.Flask(__name__)\n",
        "\n",
        "# load the pre-trained Keras model\n",
        "def define_model():\n",
        "    global model\n",
        "    model = load_model('Cloud_model.h5')\n",
        "    return print(\"Model Loaded\")\n",
        "\n",
        "# define anomaly threshold\n",
        "limit = 0.275\n",
        "\n",
        "# this method processes any requests to the /submit endpoint\n",
        "@app.route(\"/submit\", methods=[\"POST\"])\n",
        "def submit():\n",
        "    # initialize the data dictionary that will be returned in the response\n",
        "    data_out = {}\n",
        "\n",
        "    # load the data file from our endpoint\n",
        "    if flask.request.method == \"POST\":\n",
        "\n",
        "        # read the data file\n",
        "        file = flask.request.files[\"data_file\"]\n",
        "        if not file:\n",
        "            return \"No file submitted\"\n",
        "        data = []\n",
        "        stream = codecs.iterdecode(file.stream, 'utf-8')\n",
        "        for row in csv.reader(stream, dialect=csv.excel):\n",
        "            if row:\n",
        "                data.append(row)\n",
        "\n",
        "        # convert input data to pandas dataframe\n",
        "        df = pd.DataFrame(data)\n",
        "        df.set_index(df.iloc[:, 0], inplace=True)\n",
        "        df2 = df.drop(df.columns[0], axis=1)\n",
        "        df2 = df2.astype(np.float64)\n",
        "\n",
        "        # normalize the data\n",
        "        scaler = joblib.load(\"./scaler_data\")\n",
        "        X = scaler.transform(df2)\n",
        "        # reshape data set for LSTM [samples, time steps, features]\n",
        "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "        # calculate the reconstruction loss on the input data\n",
        "\n",
        "        data_out[\"Analysis\"] = []\n",
        "        preds = model.predict(X)\n",
        "        preds = preds.reshape(preds.shape[0], preds.shape[2])\n",
        "        preds = pd.DataFrame(preds, columns=df2.columns)\n",
        "        preds.index = df2.index\n",
        "\n",
        "        scored = pd.DataFrame(index=df2.index)\n",
        "        yhat = X.reshape(X.shape[0], X.shape[2])\n",
        "        scored['Loss_mae'] = np.mean(np.abs(yhat - preds), axis=1)\n",
        "        scored['Threshold'] = limit\n",
        "        scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
        "\n",
        "        # determine if an anomaly was detected\n",
        "        triggered = []\n",
        "        for i in range(len(scored)):\n",
        "            temp = scored.iloc[i]\n",
        "            if temp.iloc[2]:\n",
        "                triggered.append(temp)\n",
        "        print(len(triggered))\n",
        "        if len(triggered) > 0:\n",
        "            for j in range(len(triggered)):\n",
        "                out = triggered[j]\n",
        "                result = {\"Anomaly\": True, \"value\": out[0], \"filename\": out.name}\n",
        "                data_out[\"Analysis\"].append(result)\n",
        "        else:\n",
        "            result = {\"Anomaly\": \"No Anomalies Detected\"}\n",
        "            data_out[\"Analysis\"].append(result)\n",
        "\n",
        "    # return the data dictionary as a JSON response\n",
        "    return flask.jsonify(data_out)\n",
        "\n",
        "\n",
        "# first load the model and then start the server\n",
        "# we need to specify the host of 0.0.0.0 so that the app is available on both localhost as well\n",
        "# as on the external IP of the Docker container\n",
        "if __name__ == \"__main__\":\n",
        "    print((\"Loading the AI model and starting the server...\"\n",
        "          \"Please wait until the server has fully started before submitting\"\n",
        "          \"******************************************************************\"))\n",
        "    define_model()\n",
        "    #  app.run() # outside of a Docker container\n",
        "    app.run(host='0.0.0.0')  # within a Docker container"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FROM python:3.8.5\n",
        "# WORKDIR /app\n",
        "\n",
        "# COPY requirements.txt /app\n",
        "# RUN pip install -r ./requirements.txt\n",
        "\n",
        "# COPY app.py /app\n",
        "# COPY Cloud_model.h5 /app\n",
        "# COPY scaler_data /app\n",
        "\n",
        "# #ENV FLASK_DEBUG=1\n",
        "\n",
        "# CMD [\"python\", \"app.py\"]"
      ],
      "metadata": {
        "id": "F7LQf6j1hAS3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docker build -t anomaly-cloud:latest .\n",
        "# docker run -d -p 5000:5000 anomaly-cloud\n",
        "# curl -X POST -F data_file=@day4_data.csv 'http://localhost:5000/submit'"
      ],
      "metadata": {
        "id": "391-reh6hApu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  docker container stop your_container_id"
      ],
      "metadata": {
        "id": "n_7I0L_1hJ5-"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}